The pseudo-Boolean function~\onemax counts the number of one-bits in a bit-string~$\mathbf{x}\in\{0,1\}^n$, that is, let
\begin{equation}
\onemax(\mathbf{x}):=\sum_{i=1}^n x_i\,.
\end{equation}

The following theorem can be deduced from the results and proofs in~\cite{DJWoneone}.
\begin{theorem}\label{thm:onemaxclassic}
The expected optimization times of RLS, RLS$^*$, the (1+1)~EA, and the (1+1)~EA$^*$ for minimizing \onemax are in~$\Theta(n\log\,n)$.
\end{theorem}

We show that the expected query time in the quantum version decreases only by a logarithmic factor.
\begin{theorem}\label{thm:onemaxquantum}
The expected optimization times of QLS, QLS$^*$, the (1+1)~QEA, and the (1+1)~QEA$^*$ for minimizing \onemax are in~$\Theta(n)$.
\end{theorem}

Before we prove the theorem, we state a lemma.

\begin{lemma}\label{lem:MovingAlongLayer}
Let $\xvec \in \mathcal{S}$ be an $n$-bit Blane vector with Hamming weight $k < n/2$. Then the probability that the mutation operator of the (1+1)~EA yields a vector $\yvec \neq \xvec$ of the same Hamming weight is in~$\Theta(k/n)$, where the hidden constants are independent of $k$.
\end{lemma}

\begin{proof}
Let $P$ be the probability described in the lemma. The probability that exactly one zero-bit and one one-bit are flipped gives us a lower bound
\[
P \ge k(n-k)\cdot \frac{1}{n^2}\cdot \left(1-\frac{1}{n}\right)^{n-2} \ge k\cdot \frac{n}{2}\cdot \frac{1}{n^2}\cdot \frac{1}{e} \in \Omega(k/n).
\] 

So let us turn to the upper bound. For every fixed $i\geq 1$ the probability that exactly $i$ one-bits and $i$ zero-bits are flipped is in 
\[
O\left(\binom{k}{i}\binom{n-k}{i}1/n^{2i}\right) = O\left(\left(\frac{k}{n}\right)^i\right) = O(k/n).
\]

On the other hand, by the Chernoff bound the probability that more than $i$ bits are flipped is exponentially small in $i$. Therefore, when we sum up over all $i$ then only finitely many terms will contribute asymptotically. Hence, 
\[
P\in O(k/n),
\]
as required.


\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:onemaxclassic}]
%We start with the progressive selection strategy.
%
%The upper bound is trivial, as the QSH is always asymptotically at least as fast as the corresponding RSH.
%
%For the lower bound, we partition the search space into two sets 
%\[
%S_1 := \left\{\xvec \mid \onemax(\xvec) < \frac{2}{3}n \right\},
%\]
%\[
%S_2 := \left\{\xvec \mid \onemax(\xvec) \geq \frac{2}{3}n \right\}.
%\]
%
%Let $N(\xvec)$ be the random variable that assigns to each run the number of occurrences of $\xvec$ in the trajectory. For $i\in\{1,2\}$, let 
%\[
%\TRSH^{(i)}  = \sum_{\mathbf{x}\in\mathcal{S}_i} \EXP[N(x)] \cdot \left(p_{\mathbf{x}}\right)^{-1}
%\]
%and 
%\[
%\TQSH^{(i)}  = \sum_{\mathbf{x}\in\mathcal{S}_i} \EXP[N(x)] \cdot \left(p'_{\mathbf{x}}\right)^{-1},
%\]
%be the estimated optimization time that RLS, or the (1+1)~EA, or QLS, or the (1+1)~QEA spends in region $S_i$ (cf. Lemma~\ref{lem:regions}). 
%
%The proofs in~\cite{DJWoneone} imply that $\TRSH^{(1)} \in \Theta(n^2)$ for both RLS and the (1+1)~EA.
%
%\johannes{Actually, I haven't checked it yet, but it is clearly true since $2n/3 \cdot \sum_{i=n/2}^{2n/3} n/i = 2n/3 \cdot \log(2n/3 - n/2) = \Theta(n \log\, n)$. The start at $i=n/2$ is because the start vector has this fitness $\pm \epsilon\cdot n$ a.a.s.}
%

  Instead of analyzing the quantum algorithms directly, we analyze the
  scaled Markov chain defined in Theorem~\ref{thm:scaling}. So let $P
  = (p(\xvec,\yvec))_{\xvec,\yvec\in\mathcal{S}}$ be the transition
  matrix associated with the RSH, and let $P' =
  (p'(\xvec,\yvec))_{\xvec,\yvec\in\mathcal{S}}$ be the scaled
  transition matrix. Let $T'$ be the optimization time of the
  associated (scaled) RSH. Recall that by Theorem~\ref{thm:scaling},
  \[
  \TQSH = \EXP[T'].
  \] 

  First some notations that we use in this proof for both QLS and QEA.
  Let $\{\xvec[k]\}_{k=0}^K$ be a trajectory generated by $P'$ and let
  $b_k$ be the random variable defined as $b_k= \onemax(\xvec[k])$ be
  the number of one-bits in~$\xvec[k]$. For $k\geq 1$, let $\SCal_k$
  be the set of all search points that are acceptable from
  $\xvec[k-1]$ by the selection rule.


  Consider QLS. The mutation operator can never generate a search
  point of equal fitness. As a result both the progressive and the
  conservative variants behave identically.  In the case of local
  search, elements of $\SCal_k$ are precisely those that are obtained
  from $\xvec[k-1]$ by flipping a one-bit. Therefore,~$\SCal_k$
  contains exactly $b_{k-1}$ elements, one for every one-bit
  in~$\xvec[k-1]$. Hence, the probability
  that~$\Mut{\xvec[k-1]}\in\SCal_k$ equals $p_{\xvec[k-1]} =
  b_{k-1}/n$. Furthermore, since exactly one one-bit is flipped, we
  have~$b_k=b_{k-1}-1$. Recursively, we see~$b_k =b_0-k$, and thus
  $K=b_0$. Thus
  \[
  \EXP[T' \mid b_0] =
  \sum_{k=1}^{b_0}p_{\xvec[k-1]}^{-\nicefrac{1}{2}} =
  \sum_{k=1}^{b_0}\sqrt{\frac{n}{b_0-k+1}} =
  \sum_{t=1}^{b_0}\frac{\sqrt{n}}{\sqrt{k}}\,.
  \]

  We now replace the summation $\sum_{k=1}^{b_0}k^{-\nicefrac{1}{2}}$
  by the integral $\int_1^{b_0}x^{-\nicefrac{1}{2}}dx$ in the above
  equation. As a result we obtain $\EXP[\TQSH\mid
  b_0]\in\Theta(\sqrt{b_0\,n})$.  Recall that the initial solution
  $\xvec[0]$ is chosen uniformly at random from~$\{0,1\}^n$ and hence
  $b_0\geq n/2$~with probability at least $1/2$. Therefore, $\EXP[T']
  = \Omega(n)$. On the other hand, $b_0 \le n$, and so $\EXP[T'] =
  \Oh(n)$.


  Now let us turn to the (1+1)~QEA and the (1+1)~QEA$^*$. Let~$k\ge
  1$. We first prove that both in progressive and the conservative
  case, the progress probability $p_{\xvec[k-1]}$ is
  $\Theta(b_{\xvec}/n)$.

  For the conservative selection rule note that all vectors
  in~$\SCal_k$ are obtained from~$\xvec[k-1]$ by flipping at least one
  one-bit. For $\xvec[k]$ to be acceptable vis-a-vis $\xvec[k-1]$ at
  lease one of the bits of $\xvec[k]$ has to be flipped. Therefore, by
  the union bound
  \[ p_{\xvec[k]}\le\frac{ b_{k-1}}{n}.\]

  Conversely, the set $\SCal_k$ contains at least all those Boole's
  vectors that are obtained from $\xvec[k-1]$ by flipping a one-bit
  and not flipping any other bit. If we fix a one-bit, then the
  probability for this event to happen is
  \[
  \frac{1}{n}\Big(1-\frac{1}{n}\Big)^{n-1} \geq \frac{1}{\euler\,n}
  \]
  Since we have exactly $b_{k-1}$ one-bits, we obtain the lower bound
  \[
  p_{\xvec[k]}\ge\frac{b_{k-1}}{\euler\,n}.
  \]
  
  Consider the progressive variant of the algorithm. It follows from
  Lemma~\ref{lem:MovingAlongLayer} that the probability to move along
  a fitness layer is in~$\Oh(b_k/n)$, where the hidden constants do
  not depend on $b_k$. Therefore, the progress probability $p_{\xvec}$
  at a point~$\xvec$ with $b_{\xvec}$ one-bits is still
  in~$\Theta(b_{\xvec}/n)$. 


  We now claim that the optimization time of the progressive and the
  conservative variants differ at most by a constant factor. For this
  compare scaled Markov chains corresponding to the (1+1)~QEA and the
  the (1+1)~QEA$^*$, respectively. The (scaled) progress probabilities
  differ by at most constant factors $c$ and $C$. The conditional
  probabilities to move to a higher fitness level subject to the event
  that the fitness is increased is equal for both Markov chains. On
  the other hand, by symmetry of the objective function, it does not
  matter at which particular point of a fitness layer the algorithms
  are. Therefore, the expected optimization times differ at most by
  the factors $c$ and $C$. Hence from no on we concentrate only on the
  conservative case.


% Hence,
% \begin{equation*}
% \sum_{k=1}^{K}\sqrt{\frac{n}{b_{k-1}}}\le \EXP[T']\le\sum_{k=1}^{K}\sqrt{\frac{\euler\,n}{b_{k-1}}}\,,
% \end{equation*}
% that is,
% \begin{equation}
% \label{eq:onemaxbounds}
% \EXP[T']\in\Theta\big(\sqrt{n}\cdot\EXP\big[\sum_{k=1}^{K}b_{k-1}^{-\nicefrac{1}{2}}\big]\big).
% \end{equation}

  From Lemma~\ref{lemma:TRSHexpand}, the optimization time is the
  expected value of $\sum_k \frac{1}{\sqrt{p_{\xvec[k]}}}$ where
  $\xvec$ varies over all open trajectories. Since $p_{\xvec[k]}$ is
  $\Theta\left(\frac{b_k}{n}\right)$, the optimization time is the
  expectation of the summation $\sum_k \sqrt{\frac{n}{b_k}}$.
  Unfortunately, unlike for the QLS, we may not assume anymore that
  $b_k = b_{k-1}-1$. So we define another auxiliary random variable
  $\delta_{\ell}$ which is the $1$ if there exists a $k$ such that
  $b_k = \ell$ and $0$ otherwise. As a result we have the equation
  \begin{equation}\label{eqn:TQSHdelta}
  \TQSH = E \left[\sum_k \sqrt{\frac{n}{b_k}}\right] = \sum_{\ell=1}^n
  E[\delta_\ell] \cdot \sqrt{\frac{n}{\ell}} = \sqrt{n} \sum_{\ell=1}^n
  \frac{E[\delta_\ell]}{\sqrt{\ell}}.
  \end{equation}
  This is because for a conservative selection strategy no the $b_k$'s
  are all distinct for a particular trajectory.

  First we dispose of the upper bound of $\TQSH$. Notice that
  $E[\delta_{\ell}] \leq 1$ as it is a $0$-$1$ random variable. Hence
  we have $\TQSH \leq \sqrt{n} \sum_{\ell=1}^n \frac{1}{\sqrt{\ell}} =
  O(n)$.

  Next, we show the corresponding lower bound. Let
  \[L:=\{\ell\in\{1,\dots,n\}\colon \delta_\ell = 1\}.\]

We may expect that the values in~$L$ are fairly evenly distributed. Nevertheless, we pessimistically assume that $L=\{n-|L|+1,\dots,n\}$ in order to bound $\sum_{l\geq 1}\delta_\ell\ell^{-\nicefrac{1}{2}}$. Again, $\sum_{\ell=n-|L|+1}^n\ell^{-\nicefrac{1}{2}}\in\Omega\left(\int_{n-|L|+1}^n x^{-\nicefrac{1}{2}}dx\right)$, and thus
\begin{equation}
\label{eq:onemaxc}
   \TQSH\in\Omega\left(\sqrt{n}\cdot\EXP\big[\sqrt{n}-\sqrt{n-|L|+1}\,\big]\right)\,.
\end{equation}
To bound the size of~$L$, we consider the expected difference between~$b_{k-1}$ and~$b_k$. Conditioned on the event that at least one of the one-bits in~$\xvec[k-1]$ flips, the difference~$b_{k-1}-b_{k}$ is at most the number of further one-bits in~$\xvec[k-1]$ that flip. Thus,
\begin{align*}
\EXP[b_{k-1}-b_{k}]&\le 1+\frac{b_{k-1}-1}{n}\le 2\quad\textnormal{and} \\
\EXP[b_0-b_k]&=\sum_{i=1}^{k}\EXP[b_{i-1}-b_i]\le 2k\,.
\end{align*}

Hence, $\EXP[b_0-b_r]\le b_0/2$ for~$r=\lfloor b_0/4\rfloor$. By
Markov's In\-equality \cite{MitzemacherU05}, this implies that
$b_0-b_r<b_0$, that is,~$b_r>0$ with probability at least~$1/3$. Thus
with probability at least~$1/3$ it holds that~$|L|\ge r+1$, since the
values~$b_0,\hdots,b_r$ all differ.

Recall that with probability at least $1/2$, $b_0\ge n/2$ Thus, with a
probability bounded away from zero by a positive constant, $|L|\ge
n/6$. Now we can infer $\TQSH\in\Omega(n)$ by substituting this result
in~(\ref{eq:onemaxc}). As we have already show that $\TQSH\in O(n)$ we
have the theorem.
\end{proof}
